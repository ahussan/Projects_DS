---
title: "DATA 621 Homework 3"
author: "Critical Thinking Group 1"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  pdf_document:
    toc: yes
    toc_depth: 4
  html_document:
    toc: yes
    toc_depth: '2'
    smart: no
  word_document:
    toc: yes
    toc_depth: '2'
---
\pagebreak
\begin{center}
\bigskip
\bigskip
\bigskip
Prepared for:\\
\medskip
Prof. Dr. Nasrin Khansari\\
\smallskip
City University of New York, School of Professional Studies - Data 621\\
\bigskip
DATA 621 â€“ Business Analytics and Data Mining\\
\medskip
Home Work 3\\
\medskip
Prepared by:\\
\medskip
Critical Thinking Group  1\\ 
\medskip
Vic Chan\\ 
\smallskip
Gehad Gad\\
\smallskip
Evan McLaughlin\\  
\smallskip
Bruno de Melo\\
\smallskip
Anjal Hussan\\
\smallskip
Zhouxin Shi\\
\smallskip
Sie Siong Wong\\
\end{center}

\pagebreak


```{R message=FALSE, warning=FALSE, echo=FALSE}
if (!require('ggplot2')) (install.packages('ggplot2'))
if(!require('corrplot')) (install.packages("corrplot"))
if(!require('corrgram')) (install.packages("corrgram"))
if(!require('Hmisc')) (install.packages("Hmisc"))
if(!require('DataExplorer')) (install.packages("DataExplorer"))
if(!require('MASS')) (install.packages('MASS'))
if(!require('alr4')) (install.packages('alr4'))
if(!require('e1071')) (install.packages('e1071'))
if(!require('caret')) (install.packages('caret'))
if(!require('pROC')) (install.packages('pROC'))
if(!require('kableExtra')) (install.packages('kableExtra'))
if(!require('dplyr')) (install.packages('dplyr'))
if(!require('gridExtra')) (install.packages('gridExtra'))
```


\newpage

# Introduction

Crime has a high cost to all parts of society and it can have severe long term impact on neighborhoods. If crime rises in the neighborhood, it affects the neighborhood. Additionally, crime can even have a health cost to the community in that the perception of a dangerous neighborhood was associated with significantly lower odds of having high physical activity among both men and women. It is important to understand the propensity for crime levels of a neighborhood before investing in that neighborhood. 

# Statement of the Problem

The purpose of this report is to develop a binary logistic regression model to determine if the neighborhood will be at risk for high crime level.

# Data Exploration  

```{r echo=FALSE}

train <- read.csv("https://raw.githubusercontent.com/ahussan/DATA_621_Group1/main/HW3/crime-training-data_modified.csv", header=TRUE, sep=",")
```


Let's take a look to the first few rows of our train data set
```{r, echo=FALSE}
print(head(train, 10))
```
Looks like all the columns are numerical. The target variable is a binary variable indicating if the crime rate above the median rate (1) or not (0)

### Means
Column means of our train data set are as follows: 
```{r, echo=FALSE}
print(colMeans(train))

```

### Standard Deviation
Now let's take a look at the standard deviation of our predictor variables:
```{r, echo=FALSE}
print(apply(train, 2, sd))

```

### Median Value
Let's take a look at the median value of our predictor variables:
```{r, echo=FALSE}
print(apply(train, 2, median))

```

### Bar chart or box plot
```{r, echo=FALSE}
boxplot(train, use.cols = TRUE)

```

### Correlation matrix

```{r, echo=FALSE}
train.cor = cor(train)
print(train.cor)
```


```{r, echo=FALSE}
corrgram(train, order=TRUE, lower.panel=panel.shade,
  upper.panel=panel.pie, text.panel=panel.txt,
  main="visualize the data in correlation matrices ")

```

### Compare Target in Training

We make sure there are no issues with an inappropriate distribution of the target variable in our training data.

```{r, echo=FALSE, warning=FALSE}
knitr::kable(table(train$target))
```

### Histogram of Variables

```{r, echo=FALSE, warning=FALSE}
plot_histogram(train)
relationships <- train
relationships$chas <- NULL
pairs(train %>% select_if(is.numeric))
```
Now that we have a basic familiarity with our data, we can analyze the relationship between the numeric variables we've brought in and the target variable. We can employ boxplots and a correlation matrix to quickly analyze this, including paired plots of the numeric feature variables. 

```{r, echo=FALSE, warning=FALSE}

#convert features to factor and add a dataset feature
train$chas <- as.factor(train$chas)
train$target <- as.factor(train$target)
train$dataset <- 'train'

plotfontsize <- 8
train_int_names <- train %>% select_if(is.numeric)
int_names <- names(train_int_names)

for (i in int_names) {
  assign(paste0("var_",i), ggplot(train, aes_string(x = train$target, y = i)) + 
          geom_boxplot(color = 'steelblue', 
                       outlier.color = 'firebrick', 
                       outlier.alpha = 0.35) +
#scale_y_continuous
          labs(title = paste0(i,' vs target'), y = i, x= 'target') +
          theme_minimal() + 
          theme(
            plot.title = element_text(hjust = 0.45),
            panel.grid.major.y =  element_line(color = "grey", linetype = "dashed"),
            panel.grid.major.x = element_blank(),
            panel.grid.minor.y = element_blank(),
            panel.grid.minor.x = element_blank(),
            axis.ticks.x = element_line(color = "grey"),
            text = element_text(size=plotfontsize)
          ))
}
gridExtra::grid.arrange(var_age, var_dis, var_indus,var_lstat,
                        var_medv,var_nox,var_ptratio,var_rad, 
                        var_rm, var_tax, var_zn, nrow=4)

numeric_values <- train %>% select_if(is.numeric)
train_cor <- cor(numeric_values)
corrplot.mixed(train_cor, tl.col = 'black', tl.pos = 'lt')
```

There are a couple of items to note in the above graphics. First, in the boxplots, we observe many outliers, which could impact our regression, limiting its predictive value. Age, nox, and dis all appear to be highly correlated with our target, and numerous other features appear to have some weaker correlative relationship. Now that we've assessed the relationship between our features and the target, we can take a quick look, through our correlation matrix, at the relationship between the variables themselves. Our correlation matrix makes clear that multicollinearity is a potential issue within our observations, and we need to keep this in mind as we create and select our models.


# Data Preparation

Looking at the results from our chas variable it doesn't seem to be needed here so we can remove it.  
```{r}
test_url <- 'https://raw.githubusercontent.com/ahussan/DATA_621_Group1/main/HW3/crime-evaluation-data_modified.csv'
test <- read.csv(test_url, header=TRUE)
trainchas <- as.factor(train$chas)
train$chas <- NULL
traintarget <- as.factor(train$target)
train$target <- traintarget
testchas <- as.factor(test$chas)
test$chas <- NULL
```
## Indus  
We see a lot of outliers in the indus variable, so we'll removed the rows which indus is greater than 20 and target is 0.  
```{r}
attach(train)
p0 <- ggplot(train, aes(factor(target), indus)) + geom_boxplot()
train <- train[-which(target==0 & indus > 20),]
p1 <- ggplot(train, aes(factor(target), indus)) + geom_boxplot()
grid.arrange(p0, p1,ncol=2,nrow=1)
detach(train)
```
## Dis  
Dis also has some outliers so we'll remove rows where dis was greater than 11 and target was 0, and where dis was greater than 7.5 and target was 1.  
```{r}
attach(train)
p0 <- ggplot(train, aes(factor(target), dis)) + geom_boxplot()
train <- train[-which(target==0 & dis > 11),]
train <- train[-which(target==1 & dis > 7.5),]
p1 <- ggplot(train, aes(factor(target), dis)) + geom_boxplot()
grid.arrange(p0, p1, ncol=2,nrow=1)
detach(train)
```
## Data Summary  
Let's take a quick look at what variables we have remaining.  
```{r}
names(train)
dim(train)
```

# Build Models

### Model 1 - All Variables

First we will be creating a model with all the variables in the original dataset to create a baseline for other models. Based on the p-values results from this model we will be able to eliminate variables with large p-values

```{r}
m1 = glm(target ~ zn + indus + nox + rm + age + dis + rad + tax + ptratio + lstat + medv, data=train, family=binomial)
summary(m1)
```

We can see that variables *indus*, *nox*, *age*, *rad*, *tax*, *ptratio*, *lstat*, and *medv* have p values that are close to and or smaller than 0.05 which will be used in the next model

### Model 2 - Hand Pick Model

```{r}
m2 = glm(target ~ indus + nox + age + rad + tax + ptratio + lstat + medv, data=train, family=binomial)
summary(m2)
```

### Model 3 - Backward Step Model
We will now build a model using backwards selection in order to compare if using backwards selection is better than hand picking values to create a model
In order to create the the backward step model we will be using the *MASS* package which includes the *stepAIC* function. The backward step requires
us to pass a model which contains all of the predictors. Then the function will fit all the models which contains all but one of the predictors and
will then pick the best model using AIC
```{r}
m3 = stepAIC(m1, direction='backward', trace=FALSE)
summary(m3)
```

### Model 4 - Forward Step Model

We can use the same stepAIC function to build the fourth model. The forward selection approach starts from the null model and adds a variable that improves the model the most, one at a time, until the stopping criterion is met. We can see the result is different compared to the backward selection approach. We can see that the result is same as the saturated model m1.

```{r}
m4 = stepAIC(m1, direction='forward', trace=FALSE)
summary(m4)
```

### Model 5 - Stepwise step Model

We also can use the same stepAIC function to build the fifth model using stepwise regression. The stepwise regression method involves adding or removing potential explanatory variables in succession and testing for statistical significance after each iteration. At the very last step stepAIC as shown in the summary table has produced the optimal set of features {*zn*, *nox*, *age*, *dis*, *rad*, *ptratio*, *medv*}. This is exactly same result as the backward step model. 

```{r}
m5 = stepAIC(m1, direction='both', trace=FALSE)
summary(m5)
```

The analysis of deviance table shows further confirms that dropping these 4 variables  {*indus*, *chas*, *rm*, *lstat*} either in model 3 or 5 are statistically insignificant and can be dropped. 

```{r}

anova(m5,m1, test="Chi")

```

### Model 6 - Transformed Predictors Model

We do a box plot for each predictors used in model m5 to check skewness. Out of the 8 predictors, these 5 predictors {*zn*, *nox*, *rad*, *tax*, *Ptratio*} are quite skewed. Thus, we shall include log of these predictors in our logistic regression model m5 or m3.

```{r predictor transformation check2, echo=FALSE}
par(mfrow=c(2,4))
boxplot(zn~target, ylab="Zn",xlab="At Risk for High Crime? (0=No, 1=Yes)", data=train)
boxplot(nox~target, ylab="Nox",xlab="At Risk for High Crime? (0=No, 1=Yes)", data=train)
boxplot(age~target, ylab="Age",xlab="At Risk for High Crime? (0=No, 1=Yes)", data=train)
boxplot(dis~target, ylab="Dis",xlab="At Risk for High Crime? (0=No, 1=Yes)", data=train)
boxplot(rad~target, ylab="Rad",xlab="At Risk for High Crime? (0=No, 1=Yes)", data=train)
boxplot(tax~target, ylab="Tax",xlab="At Risk for High Crime? (0=No, 1=Yes)", data=train)
boxplot(ptratio~target, ylab="Ptratio",xlab="At Risk for High Crime? (0=No, 1=Yes)", data=train)
boxplot(medv~target, ylab="Medv",xlab="At Risk for High Crime? (0=No, 1=Yes)", data=train)
```

Now, we create a new model to include those log transformed predictors. We can see from the summary table the impact of including transformed predictors give lower deviance and lower AIC.

```{r}
m6 = glm(target~zn+nox+age+dis+rad+tax+ptratio+medv+log(zn+1)+log(nox)+log(rad)+log(tax)+log(ptratio),family=binomial(),data=train)
summary(m6)
```

Next we can check if the logistic regression model m6 is adequate or not by doing marginal model plots. From the figure below, it shows both the loess estimate curve and the fitted values curve are in agreement, and that indicates the model m6 is a valid model.

```{r warning=FALSE}
mmps(m6, layout=c(4,4))
```

We can further check the validity of model m6 by plotting leverage values versus standardized deviance. The average leverage is equal to (p + 1)/n = (14 + 1)/466 = 0.032. The p value here is the number of predictors from m6 including the intercept. So the usual cut-off is, 0.064, equal to twice the average leverage value. There are number of high leverage points can be seen in the figure below and can be removed at the data preparation step.

```{r echo=FALSE}
hvalues <- influence(m6)$hat
stanresDeviance <- residuals(m6)/sqrt(1-hvalues)
plot(hvalues,stanresDeviance,ylab="Standardized Deviance Residuals",xlab="Leverage Values",ylim=c(-3,4),xlim=c(-0.05,0.45))
abline(v=2*15/length(train$target),lty=2)
```


# Select Models

we will compare various metrics for all six models. We check modelsâ€™ confusion matrix, accuracy, classification error rate, precision, sensitivity, specificity, F1 score, and AUC.  

```{r echo=FALSE, message=FALSE, results=FALSE}
# comparing all models using various measures
CM1 <- confusionMatrix(as.factor(as.integer(fitted(m1) > .5)), as.factor(m1$y), positive = "1")
CM2 <- confusionMatrix(as.factor(as.integer(fitted(m2) > .5)), as.factor(m2$y), positive = "1")
CM3 <- confusionMatrix(as.factor(as.integer(fitted(m3) > .5)), as.factor(m3$y), positive = "1")
CM4 <- confusionMatrix(as.factor(as.integer(fitted(m4) > .5)), as.factor(m4$y), positive = "1")
CM5 <- confusionMatrix(as.factor(as.integer(fitted(m5) > .5)), as.factor(m5$y), positive = "1")
CM6 <- confusionMatrix(as.factor(as.integer(fitted(m6) > .5)), as.factor(m6$y), positive = "1")
```

```{r echo=FALSE, message=FALSE, results=FALSE}
Roc1 <- roc(train$target,  predict(m1, train, interval = "prediction"))
Roc2 <- roc(train$target,  predict(m2, train, interval = "prediction"))
Roc3 <- roc(train$target,  predict(m3, train, interval = "prediction"))
Roc4 <- roc(train$target,  predict(m4, train, interval = "prediction"))
Roc5 <- roc(train$target,  predict(m5, train, interval = "prediction"))
Roc6 <- roc(train$target,  predict(m6, train, interval = "prediction"))
```

```{r echo=FALSE, message=FALSE, results=FALSE}
metrics1 <- c(CM1$overall[1], "Class. Error Rate" = 1 - as.numeric(CM1$overall[1]), CM1$byClass[c(1, 2, 5, 7)], AUC = Roc1$auc)
metrics2 <- c(CM2$overall[1], "Class. Error Rate" = 1 - as.numeric(CM2$overall[1]), CM2$byClass[c(1, 2, 5, 7)], AUC = Roc2$auc)
metrics3 <- c(CM3$overall[1], "Class. Error Rate" = 1 - as.numeric(CM3$overall[1]), CM3$byClass[c(1, 2, 5, 7)], AUC = Roc3$auc)
metrics4 <- c(CM4$overall[1], "Class. Error Rate" = 1 - as.numeric(CM4$overall[1]), CM4$byClass[c(1, 2, 5, 7)], AUC = Roc4$auc)
metrics5 <- c(CM5$overall[1], "Class. Error Rate" = 1 - as.numeric(CM5$overall[1]), CM5$byClass[c(1, 2, 5, 7)], AUC = Roc5$auc)
metrics6 <- c(CM6$overall[1], "Class. Error Rate" = 1 - as.numeric(CM6$overall[1]), CM6$byClass[c(1, 2, 5, 7)], AUC = Roc6$auc)
```

```{r echo=FALSE}
kable(cbind(metrics1, metrics2, metrics3, metrics4, metrics5, metrics6), col.names = c("Model 1", "Model 2", "Model 3", "Model 4", "Model 5", "Model 6"))  %>% 
  kable_styling(full_width = T)
```


Model 6 performs the highest in all metrics except Class. Error Rate. 

Model 1 and 4 perform the same. 

Model 3 and 5 perform the same.

Model 5 is pretty much close to all other metrics.

Let's look at the roc curve to help us make the best selection.

```{r echo=FALSE, message=FALSE}
# plotting roc curve of model 6
plot(roc(train$target,  predict(m6, train, interval = "prediction")), print.auc = TRUE, main='ROC Curve Model 6')
```


```{r echo=FALSE, message=FALSE}
# plotting roc curve of model 5
plot(roc(train$target,  predict(m5, train, interval = "prediction")), print.auc = TRUE, main='ROC Curve Model 5')
```


```{r echo=FALSE, message=FALSE}
# plotting roc curve of model 4
plot(roc(train$target,  predict(m4, train, interval = "prediction")), print.auc = TRUE, main='ROC Curve Model 1')
```

As we can see, the model 6 is the best model. Let's now using the evaluation dataset to evaluate the model.

```{r predict evaluation dataset, echo=FALSE, message=FALSE}
evaluation <- read.csv("https://raw.githubusercontent.com/ahussan/DATA_621_Group1/main/HW3/crime-evaluation-data_modified.csv", header=TRUE, sep=",")
evaluation$TARGET <- predict(m6, evaluation, type="response")
evaluation$TARGET <- ifelse(evaluation$TARGET > 0.5, 1, 0)
print(head(evaluation,10))
```

```{r export_eval_target}
write.csv(evaluation$TARGET,paste0(getwd(),"/Evaluation_Target.csv"),row.names = FALSE)
```

# Appendix

```{r echo=TRUE, eval=FALSE}
train <- read.csv("https://raw.githubusercontent.com/ahussan/DATA_621_Group1/main/HW3/crime-training-data_modified.csv", header=TRUE, sep=",")
```

```{r echo=TRUE, eval=FALSE}
print(head(train, 10))
```

```{r means, echo=TRUE, eval=FALSE}
print(colMeans(train))
```

```{r standard deviation, echo=TRUE, eval=FALSE}
print(apply(train, 2, sd))
```

```{r medians, echo=TRUE, eval=FALSE}
print(apply(train, 2, median))
```

```{r boxplot, echo=TRUE, eval=FALSE}
boxplot(train, use.cols = TRUE)
```

```{r correlation matrix , echo=TRUE, eval=FALSE}
train.cor = cor(train)
print(train.cor)
```

```{r correlation matrix chart, echo=TRUE, eval=FALSE}
corrgram(train, order=TRUE, lower.panel=panel.shade,
  upper.panel=panel.pie, text.panel=panel.txt,
  main="visualize the data in correlation matrices ")
```

```{r target variable frequency table, echo=TRUE, eval=FALSE}
knitr::kable(table(train$target))
```

```{r histogram, echo=TRUE, eval=FALSE}
plot_histogram(train)
relationships <- train
relationships$chas <- NULL
pairs(train %>% select_if(is.numeric))
```

```{r target variable vs each predictor box plot, echo=TRUE, eval=FALSE}
#convert features to factor and add a dataset feature
train$chas <- as.factor(train$chas)
train$target <- as.factor(train$target)
train$dataset <- 'train'
plotfontsize <- 8
train_int_names <- train %>% select_if(is.numeric)
int_names <- names(train_int_names)
for (i in int_names) {
  assign(paste0("var_",i), ggplot(train, aes_string(x = train$target, y = i)) + 
          geom_boxplot(color = 'steelblue', 
                       outlier.color = 'firebrick', 
                       outlier.alpha = 0.35) +
#scale_y_continuous
          labs(title = paste0(i,' vs target'), y = i, x= 'target') +
          theme_minimal() + 
          theme(
            plot.title = element_text(hjust = 0.45),
            panel.grid.major.y =  element_line(color = "grey", linetype = "dashed"),
            panel.grid.major.x = element_blank(),
            panel.grid.minor.y = element_blank(),
            panel.grid.minor.x = element_blank(),
            axis.ticks.x = element_line(color = "grey"),
            text = element_text(size=plotfontsize)
          ))
}
gridExtra::grid.arrange(var_age, var_dis, var_indus,var_lstat,
                        var_medv,var_nox,var_ptratio,var_rad, 
                        var_rm, var_tax, var_zn, nrow=4)
numeric_values <- train %>% select_if(is.numeric)
train_cor <- cor(numeric_values)
corrplot.mixed(train_cor, tl.col = 'black', tl.pos = 'lt')
```

```{r chas variable removal, echo=TRUE, eval=FALSE}
test_url <- 'https://raw.githubusercontent.com/ahussan/DATA_621_Group1/main/HW3/crime-evaluation-data_modified.csv'
test <- read.csv(test_url, header=TRUE)
trainchas <- as.factor(train$chas)
train$chas <- NULL
traintarget <- as.factor(train$target)
train$target <- traintarget
testchas <- as.factor(test$chas)
test$chas <- NULL
```
 
```{r indus variable outliers removal, echo=TRUE, eval=FALSE}
attach(train)
p0 <- ggplot(train, aes(factor(target), indus)) + geom_boxplot()
train <- train[-which(target==0 & indus > 20),]
p1 <- ggplot(train, aes(factor(target), indus)) + geom_boxplot()
grid.arrange(p0, p1,ncol=2,nrow=1)
detach(train)
```
  
```{r dis variable outliers removal, echo=TRUE, eval=FALSE}
attach(train)
p0 <- ggplot(train, aes(factor(target), dis)) + geom_boxplot()
train <- train[-which(target==0 & dis > 11),]
train <- train[-which(target==1 & dis > 7.5),]
p1 <- ggplot(train, aes(factor(target), dis)) + geom_boxplot()
grid.arrange(p0, p1, ncol=2,nrow=1)
detach(train)
```
 
```{r cleaned dataset summary, echo=TRUE, eval=FALSE}
names(train)
dim(train)
```

```{r model 1, echo=TRUE, eval=FALSE}
m1 = glm(target ~ zn + indus + nox + rm + age + dis + rad + tax + ptratio + lstat + medv, data=train, family=binomial)
summary(m1)
```

```{r model 2, echo=TRUE, eval=FALSE}
m2 = glm(target ~ indus + nox + age + rad + tax + ptratio + lstat + medv, data=train, family=binomial)
summary(m2)
```

```{r model 3, echo=TRUE, eval=FALSE}
m3 = stepAIC(m1, direction='backward', trace=FALSE)
summary(m3)
```

```{r model 4, echo=TRUE, eval=FALSE}
m4 = stepAIC(m1, direction='forward', trace=FALSE)
summary(m4)
```

```{r model 5, echo=TRUE, eval=FALSE}
m5 = stepAIC(m1, direction='both', trace=FALSE)
summary(m5)
```

```{r anova test, echo=TRUE, eval=FALSE}
anova(m5,m1, test="Chi")
```

```{r predictor transformation check, echo=FALSE}
par(mfrow=c(2,4))
boxplot(zn~target, ylab="Zn",xlab="At Risk for High Crime? (0=No, 1=Yes)", data=train)
boxplot(nox~target, ylab="Nox",xlab="At Risk for High Crime? (0=No, 1=Yes)", data=train)
boxplot(age~target, ylab="Age",xlab="At Risk for High Crime? (0=No, 1=Yes)", data=train)
boxplot(dis~target, ylab="Dis",xlab="At Risk for High Crime? (0=No, 1=Yes)", data=train)
boxplot(rad~target, ylab="Rad",xlab="At Risk for High Crime? (0=No, 1=Yes)", data=train)
boxplot(tax~target, ylab="Tax",xlab="At Risk for High Crime? (0=No, 1=Yes)", data=train)
boxplot(ptratio~target, ylab="Ptratio",xlab="At Risk for High Crime? (0=No, 1=Yes)", data=train)
boxplot(medv~target, ylab="Medv",xlab="At Risk for High Crime? (0=No, 1=Yes)", data=train)
```

```{r model 6, echo=TRUE, eval=FALSE}
m6 = glm(target~zn+nox+age+dis+rad+tax+ptratio+medv+log(zn+1)+log(nox)+log(rad)+log(tax)+log(ptratio),family=binomial(),data=train)
summary(m6)
```

```{r marginal model plots, echo=TRUE, eval=FALSE}
mmps(m6, layout=c(4,4))
```

```{r leverage values versus standardized deviance plot, echo=TRUE, eval=FALSE}
hvalues <- influence(m6)$hat
stanresDeviance <- residuals(m6)/sqrt(1-hvalues)
plot(hvalues,stanresDeviance,ylab="Standardized Deviance Residuals",xlab="Leverage Values",ylim=c(-3,4),xlim=c(-0.05,0.45))
abline(v=2*15/length(train$target),lty=2)
```

```{r confusion matrix, echo=TRUE, eval=FALSE}
# comparing all models using various measures
CM1 <- confusionMatrix(as.factor(as.integer(fitted(m1) > .5)), as.factor(m1$y), positive = "1")
CM2 <- confusionMatrix(as.factor(as.integer(fitted(m2) > .5)), as.factor(m2$y), positive = "1")
CM3 <- confusionMatrix(as.factor(as.integer(fitted(m3) > .5)), as.factor(m3$y), positive = "1")
CM4 <- confusionMatrix(as.factor(as.integer(fitted(m4) > .5)), as.factor(m4$y), positive = "1")
CM5 <- confusionMatrix(as.factor(as.integer(fitted(m5) > .5)), as.factor(m5$y), positive = "1")
CM6 <- confusionMatrix(as.factor(as.integer(fitted(m6) > .5)), as.factor(m6$y), positive = "1")
```

```{r ROC curves, echo=TRUE, eval=FALSE}
Roc1 <- roc(train$target,  predict(m1, train, interval = "prediction"))
Roc2 <- roc(train$target,  predict(m2, train, interval = "prediction"))
Roc3 <- roc(train$target,  predict(m3, train, interval = "prediction"))
Roc4 <- roc(train$target,  predict(m4, train, interval = "prediction"))
Roc5 <- roc(train$target,  predict(m5, train, interval = "prediction"))
Roc6 <- roc(train$target,  predict(m6, train, interval = "prediction"))
```

```{r metrics, echo=TRUE, eval=FALSE}
metrics1 <- c(CM1$overall[1], "Class. Error Rate" = 1 - as.numeric(CM1$overall[1]), CM1$byClass[c(1, 2, 5, 7)], AUC = Roc1$auc)
metrics2 <- c(CM2$overall[1], "Class. Error Rate" = 1 - as.numeric(CM2$overall[1]), CM2$byClass[c(1, 2, 5, 7)], AUC = Roc2$auc)
metrics3 <- c(CM3$overall[1], "Class. Error Rate" = 1 - as.numeric(CM3$overall[1]), CM3$byClass[c(1, 2, 5, 7)], AUC = Roc3$auc)
metrics4 <- c(CM4$overall[1], "Class. Error Rate" = 1 - as.numeric(CM4$overall[1]), CM4$byClass[c(1, 2, 5, 7)], AUC = Roc4$auc)
metrics5 <- c(CM5$overall[1], "Class. Error Rate" = 1 - as.numeric(CM5$overall[1]), CM5$byClass[c(1, 2, 5, 7)], AUC = Roc5$auc)
metrics6 <- c(CM6$overall[1], "Class. Error Rate" = 1 - as.numeric(CM6$overall[1]), CM6$byClass[c(1, 2, 5, 7)], AUC = Roc6$auc)
```

```{r metrics table, echo=TRUE, eval=FALSE}
kable(cbind(metrics1, metrics2, metrics3, metrics4, metrics5, metrics6), col.names = c("Model 1", "Model 2", "Model 3", "Model 4", "Model 5", "Model 6"))  %>% 
  kable_styling(full_width = T)
```

```{r ROC plot for model 6, echo=TRUE, eval=FALSE}
# plotting roc curve of model 6
plot(roc(train$target,  predict(m6, train, interval = "prediction")), print.auc = TRUE, main='ROC Curve Model 6')
```

```{r ROC plot for model 5, echo=TRUE, eval=FALSE}
# plotting roc curve of model 5
plot(roc(train$target,  predict(m5, train, interval = "prediction")), print.auc = TRUE, main='ROC Curve Model 5')
```

```{r ROC plot for model 4, echo=TRUE, eval=FALSE}
# plotting roc curve of model 4
plot(roc(train$target,  predict(m4, train, interval = "prediction")), print.auc = TRUE, main='ROC Curve Model 1')
```
```{r evaluation, echo=TRUE, eval=FALSE}
evaluation <- read.csv("https://raw.githubusercontent.com/ahussan/DATA_621_Group1/main/HW3/crime-evaluation-data_modified.csv", header=TRUE, sep=",")
evaluation$TARGET <- predict(m6, evaluation, type="response")
evaluation$TARGET <- ifelse(evaluation$TARGET > 0.5, 1, 0)
print(head(evaluation,10))
```

```{r export, echo=TRUE, eval=FALSE}
write.csv(evaluation$TARGET,paste0(getwd(),"/Evaluation_Target.csv"),row.names = FALSE)
```
